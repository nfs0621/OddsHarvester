import logging, asyncio
from .playwright_manager import PlaywrightManager
from .browser_helper import BrowserHelper
from .odds_portal_market_extractor import OddsPortalMarketExtractor
from .odds_portal_scraper import OddsPortalScraper
from .sport_market_registry import SportMarketRegistrar
from ..utils.command_enum import CommandEnum # Changed to relative
from ..utils.proxy_manager import ProxyManager # Changed to relative
import sys

logger = logging.getLogger("ScraperApp")
MAX_RETRIES = 3
RETRY_DELAY_SECONDS = 20
TRANSIENT_ERRORS = (
    "ERR_CONNECTION_RESET",
    "ERR_CONNECTION_TIMED_OUT",
    "ERR_NAME_NOT_RESOLVED",
    "ERR_PROXY_CONNECTION_FAILED",
    "ERR_SOCKS_CONNECTION_FAILED",
    "ERR_CERT_AUTHORITY_INVALID",
    "ERR_TUNNEL_CONNECTION_FAILED",
    "ERR_NETWORK_CHANGED",
    "Timeout",  # generic timeout from Playwright
    "net::ERR_FAILED",
    "net::ERR_CONNECTION_ABORTED",
    "net::ERR_INTERNET_DISCONNECTED",
    "Navigation timeout",
    "TimeoutError",
    "Target closed",
)

async def run_scraper(
    command: CommandEnum,
    match_links: list | None = None,
    sport: str | None = None,
    date: str | None = None,
    league: str | None = None,
    season: str | None = None,
    markets: list | None = None,
    max_pages: int | None = None,
    proxies: list | None = None,
    browser_user_agent: str | None = None,
    browser_locale_timezone: str | None = None,
    browser_timezone_id: str | None = None,
    target_bookmaker: str | None = None,
    scrape_odds_history: bool = False,
    headless: bool = True
) -> dict:
    """Runs the scraping process and handles execution."""
    logger.info(f"""
        Starting scraper with parameters: command={command}, match_links={match_links}, sport={sport}, date={date}, league={league},
        season={season}, markets={markets}, max_pages={max_pages}, proxies={proxies}, browser_user_agent={browser_user_agent},
        browser_locale_timezone={browser_locale_timezone}, browser_timezone_id={browser_timezone_id},
        scrape_odds_history={scrape_odds_history}, target_bookmaker={target_bookmaker}, headless={headless}"""
    )
    
    proxy_manager = ProxyManager(cli_proxies=proxies)
    SportMarketRegistrar.register_all_markets()
    playwright_manager = PlaywrightManager()
    browser_helper = BrowserHelper()
    market_extractor = OddsPortalMarketExtractor(browser_helper=browser_helper)

    scraper = OddsPortalScraper(
        playwright_manager=playwright_manager,
        browser_helper=browser_helper,
        market_extractor=market_extractor
    )

    try:
        # Check for tzdata availability and warn the user if needed
        try:
            from zoneinfo import ZoneInfo
            # Try to use America/Edmonton timezone to check if tzdata is available
            ZoneInfo("America/Edmonton")
        except ImportError:
            logger.warning("zoneinfo module not available - timezone conversion may fail")
            logger.info("Please use Python 3.9+ for best timezone support")
        except Exception as e:
            if "No time zone found with key" in str(e):
                logger.warning("WARNING: tzdata package is not installed. Timezone conversion will use UTC offsets as fallback.")
                logger.info("To fix timezone conversion issues, install tzdata: 'pip install tzdata'")
            else:
                logger.warning(f"Warning: {e}")
        
        proxy_config = proxy_manager.get_current_proxy()
        await scraper.start_playwright(
            headless=headless, 
            browser_user_agent=browser_user_agent,
            browser_locale_timezone=browser_locale_timezone,
            browser_timezone_id=browser_timezone_id,
            proxy=proxy_config
        )
        
        if match_links and sport:
            logger.info(f"""
                Scraping specific matches: {match_links} for sport: {sport}, markets={markets}, 
                scrape_odds_history={scrape_odds_history}, target_bookmaker={target_bookmaker}
            """)
            return await retry_scrape(
                scraper.scrape_matches, 
                match_links=match_links, 
                sport=sport, 
                markets=markets, 
                scrape_odds_history=scrape_odds_history, 
                target_bookmaker=target_bookmaker
            )

        if command == CommandEnum.HISTORIC:
            if not sport or not league or not season:
                raise ValueError("Both 'sport', 'league' and 'season' must be provided for historic scraping.")
            
            logger.info(f"""
                Scraping historical odds for sport={sport} league={league}, season={season}, markets={markets}, 
                scrape_odds_history={scrape_odds_history}, target_bookmaker={target_bookmaker}, max_pages={max_pages}
            """)
            return await retry_scrape(
                scraper.scrape_historic, 
                sport=sport, 
                league=league, 
                season=season, 
                markets=markets, 
                scrape_odds_history=scrape_odds_history,
                target_bookmaker=target_bookmaker,
                max_pages=max_pages
            )
        
        elif command == CommandEnum.UPCOMING_MATCHES:
            if not date:
                raise ValueError("A valid 'date' must be provided for upcoming matches scraping.")
                
            logger.info(f"""
                Scraping upcoming matches for sport={sport}, date={date}, league={league}, markets={markets}, 
                scrape_odds_history={scrape_odds_history}, target_bookmaker={target_bookmaker}
            """)
            return await retry_scrape(
                scraper.scrape_upcoming, 
                sport=sport, 
                date=date, 
                league=league, 
                markets=markets,
                scrape_odds_history=scrape_odds_history,
                target_bookmaker=target_bookmaker
            )
        
        else:
            raise ValueError(f"Unknown command: {command}. Supported commands are 'upcoming-matches' and 'historic'.")

    except Exception as e:
        logger.error(f"An error occured: {e}")
        return None

    finally:
        await scraper.stop_playwright()

async def retry_scrape(scrape_func, *args, **kwargs):
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            return await scrape_func(*args, **kwargs)
        except Exception as e:
            if any(keyword in str(e) for keyword in TRANSIENT_ERRORS):
                logger.warning(f"[Attempt {attempt}] Transient error detected: {e}. Retrying in {RETRY_DELAY_SECONDS}s...")
                await asyncio.sleep(RETRY_DELAY_SECONDS)
            else:
                logger.error(f"Non-retryable error encountered: {e}")
                raise
    logger.error("Max retries exceeded.")
    return None
